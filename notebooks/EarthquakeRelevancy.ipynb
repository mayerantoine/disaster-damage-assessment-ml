{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71fe15ec",
   "metadata": {},
   "source": [
    "## Import list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3868b657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers, callbacks,models,layers\n",
    "from keras.applications.efficientnet import EfficientNetB0\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_addons as tfa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a77376",
   "metadata": {},
   "source": [
    "## Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c33cb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "damage_path = '../data/damage_csv'\n",
    "images_path = '../data/ASONAM17_Damage_Image_Dataset/'\n",
    "IMG_SIZE = 224"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b495ca9c",
   "metadata": {},
   "source": [
    "## Not needeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8f64ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured (use `wandb login --relogin` to force relogin)\n",
      "/home/aimachine/software_installed/anaconda3/envs/ai_learning/lib/python3.9/site-packages/IPython/html.py:12: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  warn(\"The `IPython.html` package has been deprecated since IPython 4.0. \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/gparaison/earthquake_class_project/runs/17abd8b5\" target=\"_blank\">zany-monkey-13</a></strong> to <a href=\"https://wandb.ai/gparaison/earthquake_class_project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/gparaison/earthquake_class_project/runs/17abd8b5?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f4b0719b880>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "wandb.init(project=\"earthquake_class_project\", entity=\"gparaison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76f046a",
   "metadata": {},
   "source": [
    "## GPU Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7578c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-04 17:05:55.921148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-04 17:05:55.990477: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-04 17:05:55.990634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "# Uncomment those lines if you have gpu.\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e3bd06",
   "metadata": {},
   "source": [
    "## Function to create the csv files that will be used to create the dataset\n",
    "**Will create train.csv, dev.csv, test.csv files inside the event folder**\n",
    "1. A folder will be created using the event name\n",
    "2. In that folder the following files will be created.\n",
    "3. The even_list_dir arguments should contains the event folders you want to include in this study\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0a5725d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_relevance_dataset(damage_csv_path, event, event_list_dir):\n",
    "    #Create output directory\n",
    "    try:\n",
    "        os.mkdir(os.path.join(damage_csv_path, event))\n",
    "    except FileExistsError:\n",
    "        print(f\"{os.path.join(damage_csv_path, event)} is already there\")    \n",
    "\n",
    "    # Output directory\n",
    "    output_direct = os.path.join(damage_csv_path, event)\n",
    "\n",
    "    pd_dev = pd.DataFrame()\n",
    "    pd_train = pd.DataFrame()\n",
    "    pd_test = pd.DataFrame()\n",
    "    \n",
    "    #Concatenate csv files from different events in the event_list_directory\n",
    "    for folder in event_list_dir:\n",
    "        current_dir = os.path.join(damage_csv_path, folder)\n",
    "        for file in os.listdir(current_dir):        \n",
    "            if file.endswith('.csv'):\n",
    "                file_path = os.path.join(current_dir, file)\n",
    "                temp_df = pd.read_csv(file_path)\n",
    "                temp_df.columns = ['path','label']\n",
    "                if file.startswith('dev.'):                 \n",
    "                    if pd_dev.empty:\n",
    "                        pd_dev = temp_df\n",
    "                    else:\n",
    "                        pd_dev = pd_dev.append(temp_df, ignore_index=True)\n",
    "                elif file.startswith('train.'):\n",
    "                    if pd_train.empty: \n",
    "                        pd_train = temp_df\n",
    "                    else:\n",
    "                        pd_train = pd_train.append(temp_df, ignore_index=True)\n",
    "                elif file.startswith('test.'):\n",
    "                    if pd_test.empty:\n",
    "                        pd_test = temp_df\n",
    "                    else:\n",
    "                        pd_test = pd_test.append(temp_df, ignore_index=True)\n",
    "                else:\n",
    "                    pass\n",
    "    #Creating target label regarding if an image is relevant to an eartquake or not\n",
    "    pd_dev['label'] = pd_dev['label'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    pd_train['label'] = pd_train['label'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    pd_test['label'] = pd_test['label'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "    #writting the csv files to local disk\n",
    "    dev_filename = os.path.join(output_direct, \"dev.csv\")\n",
    "    if not os.access(dev_filename, os.F_OK) and len(pd_dev)>0:\n",
    "        pd_dev.to_csv(dev_filename, index=False, header=False)\n",
    "        print(f'{dev_filename} created')\n",
    "    else:\n",
    "        print(f'{dev_filename} already created or no data found')\n",
    "        \n",
    "    train_filename = os.path.join(output_direct, \"train.csv\")\n",
    "    if not os.access(train_filename, os.F_OK) and len(pd_train) > 0:\n",
    "        pd_train.to_csv(train_filename, index=False, header=False)\n",
    "        print(f'{train_filename} created')\n",
    "    else:\n",
    "        print(f'{train_filename} already created or no data found')\n",
    "\n",
    "    test_filename = os.path.join(output_direct, \"test.csv\")\n",
    "    if not os.access(test_filename, os.F_OK) and len(pd_test) > 0:\n",
    "        pd_test.to_csv(test_filename, index=False, header=False)\n",
    "        print(f'{test_filename} created')\n",
    "    else:\n",
    "        print(f'{test_filename} already created or no data found')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0364a14",
   "metadata": {},
   "source": [
    "## Function which will create the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "893cad4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-04 17:06:03.493746: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-04 17:06:03.494551: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-04 17:06:03.494758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-04 17:06:03.494885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-04 17:06:04.285633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-04 17:06:04.285785: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-04 17:06:04.285904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-04 17:06:04.286012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5517 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:03:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "IMG_SIZE = 224\n",
    "damage_path = '../data/damage_csv'\n",
    "images_path = '../data/ASONAM17_Damage_Image_Dataset/'\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "data_augmentation_layer = tf.keras.Sequential([\n",
    "                                  layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "                                  layers.RandomRotation(0.2),\n",
    "                                  layers.RandomCrop(IMG_SIZE,IMG_SIZE),\n",
    "                                  layers.RandomContrast(factor=0.8)\n",
    "])\n",
    "\n",
    "\n",
    "def create_dataset(damage_path,event,is_augment=False,batch_size=32,buffer_size=100):\n",
    "\n",
    "    label_path = os.path.join(damage_path,event)\n",
    "    \n",
    "    img_gen = ImageDataGenerator(rescale=1/255.0,)\n",
    "        \n",
    "    train_df = pd.read_csv(os.path.join(label_path,'train.csv'),header=None)\n",
    "    train_df.columns = ['path','label']\n",
    "\n",
    "    train_gen = img_gen.flow_from_dataframe(dataframe = train_df,\n",
    "                                directory=images_path,\n",
    "                                x_col='path',\n",
    "                                y_col='label',\n",
    "                                class_mode='raw',          \n",
    "                                batch_size=batch_size,\n",
    "                                target_size= (IMG_SIZE,IMG_SIZE))\n",
    "\n",
    "\n",
    "\n",
    "    valid_df = pd.read_csv(os.path.join(label_path,'dev.csv'),header=None)\n",
    "    valid_df.columns = ['path','label']\n",
    "\n",
    "    valid_gen = img_gen.flow_from_dataframe(dataframe = valid_df,\n",
    "                                directory=images_path,\n",
    "                                x_col='path',\n",
    "                                y_col='label',\n",
    "                                class_mode='raw',         \n",
    "                                batch_size=batch_size,\n",
    "                                target_size= (IMG_SIZE,IMG_SIZE))\n",
    "    \n",
    "    test_df = pd.read_csv(os.path.join(label_path,'test.csv'),header=None)\n",
    "    test_df.columns = ['path','label']\n",
    "\n",
    "    test_gen = img_gen.flow_from_dataframe(dataframe = test_df,\n",
    "                                directory=images_path,\n",
    "                                x_col='path',\n",
    "                                y_col='label',\n",
    "                                class_mode='raw',         \n",
    "                                batch_size=batch_size,\n",
    "                                target_size= (IMG_SIZE,IMG_SIZE)) \n",
    "      \n",
    "    # Now we're converting our ImageDataGenerator to Dataset\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_generator(\n",
    "            lambda: train_gen ,  # Our generator \n",
    "            output_types = (tf.float32 , tf.float32) , # How we're expecting our output dtype\n",
    "            output_shapes = ([None , IMG_SIZE , IMG_SIZE , 3] , [None , ]) # How we're expecting our output shape\n",
    "        )\n",
    "\n",
    "    valid_dataset = tf.data.Dataset.from_generator(\n",
    "            lambda: valid_gen , \n",
    "            output_types = (tf.float32 , tf.float32), \n",
    "            output_shapes = ([None , IMG_SIZE , IMG_SIZE , 3] , [None , ])\n",
    "        )\n",
    "    \n",
    "    test_dataset = tf.data.Dataset.from_generator(\n",
    "            lambda: test_gen , \n",
    "            output_types = (tf.float32 , tf.float32), \n",
    "            output_shapes = ([None , IMG_SIZE , IMG_SIZE , 3] , [None , ])\n",
    "        )\n",
    "\n",
    "    if is_augment:\n",
    "        train_dataset = train_dataset.map(lambda x,y: (data_augmentation_layer(x,training=True),y),\n",
    "                                num_parallel_calls=tf.data.AUTOTUNE)\n",
    "            \n",
    "    print(f\"steps_per_epochs: {len(train_df)// batch_size}\")\n",
    "    print(f\"validations_steps: {len(valid_df)// batch_size}\")\n",
    "        \n",
    "        \n",
    "    steps_per_epoch = len(train_df)// batch_size\n",
    "    validation_steps = len(valid_df)// batch_size\n",
    "\n",
    "\n",
    "    train_dataset = train_dataset.prefetch(buffer_size=10)\n",
    "    valid_dataset = valid_dataset.prefetch(buffer_size=10)\n",
    "\n",
    "    return train_dataset,valid_dataset,test_dataset,steps_per_epoch,validation_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7241c85a",
   "metadata": {},
   "source": [
    "## Here we decided to use those events in our study. The data from those events will be combined\n",
    "- The event name is cross_event_relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d376764e",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_list_dir = ['ecuador', 'nepal', 'gg']\n",
    "event = 'cross_event_relevance'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7935c3d6",
   "metadata": {},
   "source": [
    "## Actual creation of the dataset calling our previously defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83fe6526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/damage_csv/cross_event_relevance is already there\n",
      "../data/damage_csv/cross_event_relevance/dev.csv already created or no data found\n",
      "../data/damage_csv/cross_event_relevance/train.csv already created or no data found\n",
      "../data/damage_csv/cross_event_relevance/test.csv already created or no data found\n",
      "Found 14632 validated image filenames.\n",
      "Found 4876 validated image filenames.\n",
      "Found 4874 validated image filenames.\n",
      "steps_per_epochs: 457\n",
      "validations_steps: 152\n"
     ]
    }
   ],
   "source": [
    "create_relevance_dataset(damage_path, event, event_list_dir)\n",
    "\n",
    "train_ds, valid_ds,test_ds,steps_per_epoch,validation_steps, = create_dataset( damage_path,event,\n",
    "                                    is_augment=False,batch_size=32,buffer_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c8b20f",
   "metadata": {},
   "source": [
    "## Function to plot history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95ca42ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subplot_learning_curve_d(history):\n",
    "    #plt.clf()\n",
    "    plt.figure(figsize=(10,5))\n",
    "    for i,metric in enumerate(['acc','loss']):\n",
    "        plt.subplot(1,2,i+1)\n",
    "        plt.plot(history.history[metric])\n",
    "        plt.plot(history.history['val_' + metric])\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel(metric)\n",
    "        plt.legend((metric, 'val_' + metric))\n",
    "        plt.title(\": Learning curve \" + metric + \" vs \" + 'val_' + metric)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f9261d",
   "metadata": {},
   "source": [
    "## Effiecient Net model feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79a6a4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.efficientnet import EfficientNetB0\n",
    "def get_efficient_model(lr=0.001):\n",
    "    tf.keras.backend.clear_session()\n",
    "    print(f\"lr in model = {lr}\")\n",
    "    pre_trained_model = EfficientNetB0(include_top=False,\n",
    "                                       weights='imagenet',\n",
    "                                       input_shape=(IMG_SIZE,IMG_SIZE,3))\n",
    "\n",
    "    pre_trained_model.trainable = False\n",
    "\n",
    "    inputs = layers.Input(shape=(IMG_SIZE,IMG_SIZE,3))\n",
    "    x = tf.keras.applications.efficientnet.preprocess_input(inputs * 255.0)\n",
    "    x = pre_trained_model(x,training= False)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(256,activation='relu')(x)\n",
    "    outputs = layers.Dense(1,activation='sigmoid')(x)\n",
    "\n",
    "    efficient_model = models.Model(inputs,outputs)\n",
    "    efficient_model.compile(optimizer = optimizers.RMSprop(learning_rate=lr),\n",
    "                            loss='binary_crossentropy',\n",
    "                            metrics=['acc'])\n",
    "\n",
    "    efficient_model.summary()\n",
    "    \n",
    "    return efficient_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8129a432",
   "metadata": {},
   "source": [
    "## Running the model only Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859dd02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr in model = 0.001\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " tf.math.multiply (TFOpLambd  (None, 224, 224, 3)      0         \n",
      " a)                                                              \n",
      "                                                                 \n",
      " efficientnetb0 (Functional)  (None, 7, 7, 1280)       4049571   \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 1280)             0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1280)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               327936    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,377,764\n",
      "Trainable params: 328,193\n",
      "Non-trainable params: 4,049,571\n",
      "_________________________________________________________________\n",
      "Found 14632 validated image filenames.\n",
      "Found 4876 validated image filenames.\n",
      "Found 4874 validated image filenames.\n",
      "steps_per_epochs: 731\n",
      "validations_steps: 243\n",
      "Epoch 21/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-04 17:06:47.530007: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8302\n",
      "2022-02-04 17:06:48.966352: W tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2022-02-04 17:06:48.967404: W tensorflow/stream_executor/gpu/asm_compiler.cc:230] Falling back to the CUDA driver for PTX compilation; ptxas does not support CC 8.6\n",
      "2022-02-04 17:06:48.967413: W tensorflow/stream_executor/gpu/asm_compiler.cc:233] Used ptxas at ptxas\n",
      "2022-02-04 17:06:48.967922: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] UNIMPLEMENTED: ptxas ptxas too old. Falling back to the driver to compile.\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8/731 [..............................] - ETA: 18s - loss: 0.5663 - acc: 0.7312"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-04 17:06:49.276519: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "731/731 [==============================] - 144s 189ms/step - loss: 0.3269 - acc: 0.8644 - val_loss: 0.2802 - val_acc: 0.8860\n",
      "Epoch 22/40\n",
      "731/731 [==============================] - 132s 181ms/step - loss: 0.2712 - acc: 0.8934 - val_loss: 0.2717 - val_acc: 0.8887\n",
      "Epoch 23/40\n",
      "731/731 [==============================] - 132s 181ms/step - loss: 0.2415 - acc: 0.9053 - val_loss: 0.2828 - val_acc: 0.8914\n",
      "Epoch 24/40\n",
      "731/731 [==============================] - 136s 186ms/step - loss: 0.2212 - acc: 0.9167 - val_loss: 0.2974 - val_acc: 0.8842\n",
      "Epoch 25/40\n",
      "731/731 [==============================] - 133s 183ms/step - loss: 0.2001 - acc: 0.9238 - val_loss: 0.2980 - val_acc: 0.8953\n",
      "Epoch 26/40\n",
      "731/731 [==============================] - 131s 179ms/step - loss: 0.1873 - acc: 0.9318 - val_loss: 0.3105 - val_acc: 0.8918\n",
      "Epoch 27/40\n",
      "551/731 [=====================>........] - ETA: 23s - loss: 0.1677 - acc: 0.9388"
     ]
    }
   ],
   "source": [
    "EfficientNetB0_h = get_efficient_model()\n",
    "n_epochs = 20\n",
    "train_ds, valid_ds,test_ds,steps_per_epoch,validation_steps, = create_dataset(damage_path,event,\n",
    "                                        is_augment=False,batch_size=20,buffer_size=100)\n",
    "\n",
    "history = EfficientNetB0_h.fit(train_ds,\n",
    "                        initial_epoch=n_epochs,\n",
    "                        epochs=2*n_epochs,\n",
    "                        steps_per_epoch = steps_per_epoch,\n",
    "                        validation_data=valid_ds,\n",
    "                        validation_steps = validation_steps)\n",
    "\n",
    "\n",
    "subplot_learning_curve_d(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2498d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
